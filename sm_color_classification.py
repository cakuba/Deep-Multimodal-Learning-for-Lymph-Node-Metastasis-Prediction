from __future__ import print_function
# to filter some unnecessory warning messages
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

import cv2
import os
import pickle
import numpy as np
from shutil import copyfile
import matplotlib.pyplot as plt

# 指定GPU
os.environ["CUDA_VISIBLE_DEVICES"]="1"

# 模型训练代码
import numpy as np
from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD, Adam, Adagrad 
from tensorflow.keras.regularizers import l1, l2
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.callbacks import ReduceLROnPlateau
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

def lr_schedule(epoch):
    """Learning Rate Schedule
    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.
    # Arguments
        epoch (int): The number of epochs
    # Returns
        lr (float32): learning rate
    """
    lr = 0.0003
    if epoch > 800:
        lr *= 0.5e-3
    elif epoch > 600:
        lr *= 1e-3
    elif epoch > 400:
        lr *= 1e-2
    elif epoch > 200:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr

def classification_model(size, epochs, train_dir, valid_dir=None, batch_size=32, shuffle=True, show_summary=True, weights_path="./weights/"):
    """
    size=(XSIZE, YSIZE, ZSIZE), 给定训练图像的大小
    epochs, 训练轮数
    train_dir, 训练数据集目录地址
    valid_dir, 验证数据集目录地址

    note: 默认采用InceptionResnetV2作为模型；性能最好的模型保存在当前目录的saved_models子目录下面
    """

    # 输入模型的图像尺寸
    if len(size) > 2:
        XSIZE, YSIZE, ZSIZE = size[0], size[1], size[2]
    else:
        XSIZE, YSIZE, ZSIZE = size[0], size[1], 3

    weight_file = weights_path+"inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5"
    model_ir2 = InceptionResNetV2(include_top=False, weights=weight_file, input_shape=(XSIZE, YSIZE, ZSIZE))

    for layer in model_ir2.layers:
        layer.trainable = True
    model = GlobalAveragePooling2D(name='GlobalAverage')(model_ir2.output)
    model = Dense(256, activation='relu')(model)
    model = Dropout(0.5)(model)
    model = Dense(64, activation='relu')(model)
    model = Dense(1, activation='sigmoid')(model)
    model_ir2_sp = Model(model_ir2.input, model)
    model_ir2_sp.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['acc'])

    if show_summary:
        model_ir2_sp.summary()

    # Prepare model model saving directory.
    save_dir = os.path.join(os.getcwd(), 'saved_models')
    model_name = 'InceptionResNetV2_COLOR.{epoch:03d}.h5'
    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)
    filepath = os.path.join(save_dir, model_name)

    # Prepare callbacks for model saving and for learning rate adjustment.
    checkpoint = ModelCheckpoint(filepath=filepath,
                                 monitor='val_acc',
                                 verbose=1,
                                 save_best_only=True,
                                 save_weights_only=True)
    lr_scheduler = LearningRateScheduler(lr_schedule)
    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                                   cooldown=0,
                                   patience=10,
                                   min_lr=0.5e-6)
    callbacks = [checkpoint,lr_scheduler,lr_reducer]

    # This will do preprocessing and realtime data augmentation:
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        fill_mode='reflect',
        horizontal_flip=True,
        vertical_flip=True)   
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(XSIZE, YSIZE),
        batch_size=batch_size,
        class_mode='binary')

    if valid_dir:
        valid_datagen = ImageDataGenerator(rescale=1./255)
        # Flow validation images in batches
        valid_generator = valid_datagen.flow_from_directory(
                valid_dir,
                target_size=(XSIZE, YSIZE),
                batch_size=batch_size,
                class_mode='binary')
        n_valid_samples = valid_generator.samples
    else:
        valid_generator = None
        n_valid_samples = 0

    n_training_samples = train_generator.samples
    # Fit the model on the batches generated by datagen.flow().
    history = model_ir2_sp.fit_generator(
        train_generator,
        steps_per_epoch=int(n_training_samples/batch_size), 
        epochs=epochs,
        validation_data=valid_generator,
        validation_steps=int(n_valid_samples/batch_size),
        shuffle=shuffle,
        callbacks=callbacks,
        verbose=1)
    
    return history

size = (250,250,3)
epochs = 3000
train_dir = "./data/SingleModality/training/CDFI"
valid_dir = "./data/SingleModality/valid/CDFI"
history = classification_model(size,epochs,train_dir,batch_size=64,valid_dir=valid_dir,show_summary=True)

history_saved = True
if history_saved:
   history_file = os.path.join('./history', 'sm_color_history-epoch-'+str(epochs)+'.dat')
   with open(history_file,'wb') as f:
       pickle.dump(history.history, f)
   print("Network training history successfully saved!")

